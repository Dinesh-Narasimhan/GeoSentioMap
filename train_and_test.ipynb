{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llPSVPqMAFeS",
        "outputId": "d5804bdd-4b14-4a27-b7cb-839f0823952b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pandas as pd, torch, torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "img_root = \"/content/drive/MyDrive/GeoSentioMap/data\"\n",
        "csv_path = \"/content/drive/MyDrive/GeoSentioMap/metadata.csv\"\n",
        "\n",
        "# Emotion Mapping\n",
        "emotion_map = {'peaceful': 0, 'neutral': 1, 'energetic': 2, 'chaotic': 3}\n",
        "num_classes = len(emotion_map)\n",
        "\n",
        "# Image Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Metadata encoder\n",
        "def encode_meta(row):\n",
        "    location_map = {'kerala': 0, 'chennai': 1}\n",
        "    weather_map = {'sunny': 0, 'rainy': 1, 'cloudy': 2}\n",
        "    time_map = {'morning': 0, 'afternoon': 1, 'evening': 2}\n",
        "    return [\n",
        "        location_map.get(row['location'], 0),\n",
        "        weather_map.get(row['weather'], 0),\n",
        "        time_map.get(row['time_of_day'], 0)\n",
        "    ]\n",
        "\n",
        "# Custom Dataset\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, img_root):\n",
        "        self.df = df\n",
        "        self.img_root = img_root\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_root, row['location'], row['filename'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except:\n",
        "            image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
        "\n",
        "        image = transform(image)\n",
        "        meta = torch.tensor(encode_meta(row), dtype=torch.float32)\n",
        "        label = torch.tensor(emotion_map[row['emotion_label']], dtype=torch.long)\n",
        "        return image, meta, label\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "dataset = EmotionDataset(df, img_root)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "#  Dynamic class weight fix (based on available labels only)\n",
        "labels = df['emotion_label'].map(emotion_map).dropna().astype(int)\n",
        "unique_labels = np.unique(labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels)\n",
        "weights_tensor = torch.ones(num_classes, dtype=torch.float32)\n",
        "weights_tensor[unique_labels] = torch.tensor(class_weights, dtype=torch.float32)\n",
        "weights_tensor = weights_tensor.to(device)\n",
        "\n",
        "# Load ResNet\n",
        "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in resnet.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "resnet.fc = nn.Identity()\n",
        "\n",
        "# Final Model\n",
        "class EmotionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = resnet\n",
        "        self.meta_fc = nn.Sequential(\n",
        "            nn.Linear(3, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, meta):\n",
        "        img_feat = self.cnn(img)\n",
        "        meta_feat = self.meta_fc(meta)\n",
        "        combined = torch.cat([img_feat, meta_feat], dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# Initialize model\n",
        "model = EmotionNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "\n",
        "# üîÅ Training\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, metas, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        metas = metas.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        preds = model(imgs, metas)\n",
        "        loss = criterion(preds, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\" Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "save_path = \"/content/drive/MyDrive/GeoSentioMap/emotion_model_final.pt\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\" Final Model saved at: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9gtzsgCGrUG",
        "outputId": "5534a465-2256-418e-d930-2b6af83ca134"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 1 | Loss: 18.9345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 2 | Loss: 15.7387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 3 | Loss: 13.7354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 4 | Loss: 12.1258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 5 | Loss: 10.6793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 6 | Loss: 8.3672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 7 | Loss: 6.6520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 8 | Loss: 5.3669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 9 | Loss: 3.8040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 10 | Loss: 2.9369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 11 | Loss: 2.1803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 12 | Loss: 2.1220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 13 | Loss: 1.6320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 14 | Loss: 1.3720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 15 | Loss: 1.0857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 16 | Loss: 1.3143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 17 | Loss: 1.0441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 18 | Loss: 0.9041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 19 | Loss: 1.0061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Epoch 20 | Loss: 0.9229\n",
            "‚úÖ Final Model saved at: /content/drive/MyDrive/GeoSentioMap/emotion_model_final.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# --- 1. Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- 2. Emotion Map ---\n",
        "emotion_map = {0: 'peaceful', 1: 'neutral', 2: 'energetic', 3: 'chaotic'}\n",
        "\n",
        "# --- 3. Transforms (same as training) ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# --- 4. Metadata encoding ---\n",
        "def encode_meta(location, weather, time_of_day):\n",
        "    location_map = {'kerala': 0, 'chennai': 1}\n",
        "    weather_map = {'sunny': 0, 'rainy': 1, 'cloudy': 2}\n",
        "    time_map = {'morning': 0, 'afternoon': 1, 'evening': 2}\n",
        "    return torch.tensor([\n",
        "        location_map.get(location, 0),\n",
        "        weather_map.get(weather, 0),\n",
        "        time_map.get(time_of_day, 0)\n",
        "    ], dtype=torch.float32).to(device)\n",
        "\n",
        "# --- 5. Define model (same as training) ---\n",
        "class EmotionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in resnet.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "        resnet.fc = nn.Identity()\n",
        "\n",
        "        self.cnn = resnet\n",
        "        self.meta_fc = nn.Sequential(\n",
        "            nn.Linear(3, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, meta):\n",
        "        img_feat = self.cnn(img)\n",
        "        meta_feat = self.meta_fc(meta)\n",
        "        combined = torch.cat([img_feat, meta_feat], dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# --- 6. Load model ---\n",
        "model = EmotionNet().to(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/GeoSentioMap/emotion_model_final.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# --- 7. Load your test image ---\n",
        "image_path = \"/content/drive/MyDrive/GeoSentioMap/test4.webp\"  #  CHANGE THIS\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_tensor = transform(img).unsqueeze(0).to(device)  # shape: [1, 3, 224, 224]\n",
        "\n",
        "# --- 8. Create metadata ( adjust values) ---\n",
        "meta_tensor = encode_meta(\"kerala\", \"sunny\", \"morning\").unsqueeze(0)  # shape: [1, 3]\n",
        "\n",
        "# --- 9. Predict ---\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor, meta_tensor)\n",
        "    pred = torch.argmax(output, dim=1).item()\n",
        "    print(f\" Predicted Emotion: {emotion_map[pred]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tibOz-XP1Lm",
        "outputId": "52c930a3-b5fa-4181-c10c-9972e7a50772"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Predicted Emotion: peaceful\n"
          ]
        }
      ]
    }
  ]
}